from utils.utils import *
from pipeline.pipeline_aux import *
from dataio.cifar2loader import Cifar10Data
import utils.evalobj as ev
import models.networks as net
import utils.optimizer as op
from pipeline.training_aux import *

def training_sr_small_cnn(config_data):
	print("pipeline/training.py. training_sr_small_cnn()")
	print("sr: self-reflection")
	

	data_dir = config_data['data_directory']['cifar10']
	CFDATA = Cifar10Data()
	if DEBUG_TRAINING_SIZE_PER_RAW_BATCH is not None: CFDATA.size_per_raw_batch = DEBUG_TRAINING_SIZE_PER_RAW_BATCH
	CFDATA.load_data(config_data, as_categorical=True)
	trainloader = DataLoader(dataset=CFDATA, num_workers=0, batch_size=config_data['basic']['batch_size'], shuffle=True)
	print("  trainloader loaded")

	#################################################
	# Creating discriminator
	config_data_discriminator = create_discriminator_config(config_data)
	model_dir_discriminator = os.path.join(config_data_discriminator['working_dir'], config_data_discriminator['relative_checkpoint_dir'],config_data_discriminator['model_label_name'])
	this_discriminator_fullpath = os.path.join(model_dir_discriminator,config_data['model_label_name'] + '_discriminator.model') 
	this_discriminator = net.Discriminator()
	if os.path.exists(this_discriminator_fullpath): this_discriminator = this_discriminator.load_state(config_data_discriminator); print("Load existing Discriminator model...")
	this_discriminator.training_cycle = this_discriminator.training_cycle + 1
	this_discriminator.write_diary(config_data_discriminator)
	print("discriminator: ",end=''); net.count_parameters(this_discriminator)

	criterion_for_discriminator = nn.CrossEntropyLoss()
	optimizer_for_discriminator = op.get_optimizer_for_discriminator(this_discriminator, config_data_discriminator)
	cetracker_for_discriminator = ev.CrossEntropyLossTracker(config_data_discriminator , display_every_n_minibatchs=2)

	#################################################
	# Creating the neural network
	model_dir = os.path.join(config_data['working_dir'], config_data['relative_checkpoint_dir'],config_data['model_label_name'])
	main_model_fullpath = os.path.join(model_dir,config_data['model_label_name'] + '.model') 
	this_net = net.SmallCNN()
	if os.path.exists(main_model_fullpath): this_net = this_net.load_state(config_data); print("Load existing model...")
	this_net.training_cycle = this_net.training_cycle + 1
	this_net.write_diary(config_data)
	print("this network: ", end=''); net.count_parameters(this_net)

	criterion = nn.CrossEntropyLoss() # with MSE, criterion(...)=0 for correct prediction
	optimizer = op.get_optimizer(this_net, config_data)
	cetracker = ev.CrossEntropyLossTracker(config_data , display_every_n_minibatchs=2)

	for i_epoch in range(config_data['basic']['n_epoch']):
		"""
		See y_gt.  
		In a sense, D is predicting the loss generated by this_net
		y_gt is the ground truth of this loss
		"""
		this_discriminator.start_timer()

		######################################################################
		# Training discriminator
		######################################################################
		for i, data in enumerate(trainloader, 0):
			if DEBUG_TRAINING_LOOP_SKIP_DISCRIMINATOR: continue
			optimizer_for_discriminator.zero_grad()
			x = data[0].to(this_device).to(torch.float) # interp3d() resizes only x. labels remains in the original shape.
			
			#############################################################
			# part 1(A) real data
			#############################################################
			labels = data[1].to(this_device).to(torch.float) # [0] compare with part 1(B). e.g. torch.Size([batch_size, 10])
					
			if not DEBUG_TRAINING_LOOP_SKIP_DISCRIMINATOR_PART1:
				xD = (x,labels) # [1] compare with part 1(B)
				if DEBUG0002_training_sr_small_cnn(DEBUG_TRAINING_LOOP, \
					this_discriminator, xD, labels): break 
				outputs = this_discriminator(xD).contiguous() # [2] compare with part 1(B) 
				# outputs = adhoc_normalization(outputs)
				y_gt = torch.ones(outputs.shape[0]).to(this_device) # [3] compare with part 1(B)
				loss_discriminator = criterion_for_discriminator(outputs, y_gt.to(torch.int64))
				loss_discriminator.backward()

			#############################################################
			# part 1(B) fake data
			#############################################################

			x_fake ,y_fake = training_sr_small_cnn_prepare_fake_prediction(x, labels)
			xD_fake = (x_fake.to(this_device).to(torch.float), y_fake.to(this_device).to(torch.float)) # [1] compare with part 1(A)
			outputs_fake = this_discriminator(xD_fake).contiguous() # [2] compare with part 1(A)
			# outputs_fake = adhoc_normalization(outputs_fake)
			y_gt = torch.zeros(outputs_fake.shape[0]).to(this_device)
			if DEBUG0001_training_sr_small_cnn(\
				DEBUG_TRAINING_LOOP,x, this_net, labels, y_fake, outputs_fake, y_gt): break		
			loss_discriminator = criterion_for_discriminator(outputs_fake, y_gt.to(torch.int64))
			loss_discriminator.backward()

			#############################################################
			# Finally, update discriminator
			#############################################################
			cetracker_for_discriminator.store_loss(loss_discriminator)
			optimizer_for_discriminator.step()

		this_discriminator.stop_timer()
		if not DEBUG_TRAINING_LOOP_SKIP_DISCRIMINATOR:
			if DEBUG_TRAINING_LOOP: break
			this_discriminator.latest_epoch = this_discriminator.latest_epoch + 1
			this_discriminator.write_diary_post_epoch(config_data_discriminator ,no_of_data_processed=str(i+1) )
			this_discriminator.save_models(this_discriminator, config_data_discriminator)
			this_discriminator.clear_up_models(this_discriminator,config_data_discriminator, keep_at_most_n_latest_models=config_data_discriminator['basic']['keep_at_most_n_latest_models'])

		######################################################################
		# Training generator/ main network
		######################################################################
		this_discriminator.eval()
		if not DEBUG_TRAINING_DO_NOT_TRAIN_GENERATOR:
			this_net.start_timer()		
			for i, data in enumerate(trainloader, 0):
				optimizer.zero_grad()
				x = data[0].to(this_device).to(torch.float) # interp3d() resizes only x. labels remains in the original shape.
				labels = data[1].to(this_device).to(torch.float)

				if DEBUG0003_training_sr_small_cnn(DEBUG_TRAINING_LOOP, this_net, x, labels, this_discriminator): break
				if DEBUG_TRAINING_Y_RESHAPE:
					y = this_net(x).contiguous();print("  [s1] y.shape: %s\n       y = %s"%(str(y.shape), str(y))); break

				y = this_net(x).contiguous()
				yD = this_discriminator((x, y)).contiguous()
				y_gt = torch.ones(yD.shape[0]).to(this_device) 

				loss = criterion(yD, y_gt.to(torch.int64))
				cetracker.store_loss(loss)
				loss.backward()
				optimizer.step()


			this_net.stop_timer()
			if DEBUG_TRAINING_LOOP: break
			if DEBUG_TRAINING_Y_RESHAPE: break
			this_net.latest_epoch = this_net.latest_epoch + 1
			this_net.write_diary_post_epoch(config_data,no_of_data_processed=str(i+1) )

			this_net.save_models(this_net, config_data)
			this_net.clear_up_models(this_net,config_data, keep_at_most_n_latest_models=config_data['basic']['keep_at_most_n_latest_models'])
		this_discriminator.train()
		
	cetracker.save_loss_plot(label_tag = str(this_net.latest_epoch))
	cetracker_for_discriminator.save_loss_plot(label_tag = str(this_discriminator.latest_epoch))

def training_small_cnn(config_data):
	print("pipeline/training.py. training_cnn().")

	data_dir = config_data['data_directory']['cifar10']
	CFDATA = Cifar10Data()
	if DEBUG_TRAINING_SIZE_PER_RAW_BATCH is not None: CFDATA.size_per_raw_batch = DEBUG_TRAINING_SIZE_PER_RAW_BATCH
	CFDATA.load_data(config_data)

	trainloader = DataLoader(dataset=CFDATA, num_workers=0, 
		batch_size=config_data['basic']['batch_size'], shuffle=True)
	print("  trainloader loaded")

	model_dir = os.path.join(config_data['working_dir'], config_data['relative_checkpoint_dir'],config_data['model_label_name'])
	main_model_fullpath = os.path.join(model_dir,config_data['model_label_name'] + '.model') 
	this_net = net.SmallCNN()

	if os.path.exists(main_model_fullpath): this_net = this_net.load_state(config_data); print("Load existing model...")
	this_net.training_cycle = this_net.training_cycle + 1
	this_net.write_diary(config_data)
	net.count_parameters(this_net)

	criterion = nn.CrossEntropyLoss()
	optimizer = op.get_optimizer(this_net, config_data)
	cetracker = ev.CrossEntropyLossTracker(config_data, display_every_n_minibatchs=2)

	for i_epoch in range(config_data['basic']['n_epoch']):
		this_net.start_timer()
		for i, data in enumerate(trainloader, 0):
			optimizer.zero_grad()
			x = data[0].to(this_device).to(torch.float) # interp3d() resizes only x. labels remains in the original shape.
			labels = data[1].to(this_device).to(torch.float)

			if DEBUG_TRAINING_DATA_LOADER_PRINT: training_cnn_print_data(x, labels)
			if DEBUG_TRAINING_LOOP: 
				outputs = this_net.forward_debug(x);
				print("  outputs:%s outputs.shape:%s"%(str(outputs), str(outputs.shape)))
				print("  labels.to(torch.int64):%s labels.to(torch.int64).shape:%s"%(str(labels.to(torch.int64)), str(labels.to(torch.int64).shape)))
				break
			outputs = this_net(x).contiguous() 
			loss = criterion(outputs, labels.to(torch.int64))
			cetracker.store_loss(loss) # tracking progress
			loss.backward()
			optimizer.step()
		this_net.stop_timer()
		if DEBUG_TRAINING_LOOP: break
		this_net.latest_epoch = this_net.latest_epoch + 1
		this_net.write_diary_post_epoch(config_data,no_of_data_processed=str(i+1) )

		this_net.save_models(this_net, config_data)
		this_net.clear_up_models(this_net,config_data, keep_at_most_n_latest_models=config_data['basic']['keep_at_most_n_latest_models'])
	cetracker.save_loss_plot(label_tag = str(this_net.latest_epoch))

